{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "Zadanie 4. (6p) W tym zadaniu będziemy generować zdania w języku naturalnym spełniające\n",
    "dodatkową właściwość: wszystkie słowa powinny zaczynać się od tej samej litery. Na SKOS\n",
    "będzie dostępna lista prefiksów, takich jak: „Prawdziwy piekarz przyprawia pieczywo pieprzem”,\n",
    "Twoje generacje powinny zawsze zaczynać się od wylosowanego prefiksu i kończyć kropką (lub innym\n",
    "znakiem interpunkcjnym kończącym zdanie).\n",
    "Twój program powinien:\n",
    "\n",
    "1. Stosować zarówno top-k, jak i top-p.\n",
    "2. Modyfikować rozkład prawdopodobieństwa tokenów\n",
    "3. Generować „porządne” teksty, to znaczy z wyrazami składającymi się z liter, ze spacjami jako\n",
    "   separatorami, z interpunkcją poprawnie formatowaną (na przykład przecinek powinien być\n",
    "   doklejony do poprzedzającego go słowa)\n",
    "4. Unikać powtórzeń\n",
    "5. Generować kilka wariantów i wybierać z nich najlepszy, wg zdefiniowanego przez Ciebie kryterium\n",
    "   Wygenerowany tekst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "model_name = \"flax-community/papuGaPT2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Po prelekcji przewidziana'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint, seed\n",
    "\n",
    "\n",
    "def load_sample():\n",
    "    with open(\"prefiksy.txt\", \"r\", encoding=\"utf-8\") as prefixes:\n",
    "\n",
    "        lines = prefixes.readlines()\n",
    "        return list(set([line.rstrip() for line in lines]))\n",
    "\n",
    "\n",
    "def random_prefix(sample, set_seed=False):\n",
    "    if set_seed:\n",
    "        seed(0)\n",
    "    size = len(sample)\n",
    "    row_num = randint(0, size - 1)\n",
    "    # return (lines[row_num].rstrip(), row_num+1)\n",
    "    return sample[row_num]\n",
    "\n",
    "\n",
    "sample = load_sample()\n",
    "r_prefix = random_prefix(sample)\n",
    "r_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sample dataset to a file\n",
    "with open(\"training_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in sample:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "### Plik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer, file_path=file_path, block_size=block_size\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Data collator\n",
    "def data_collator(tokenizer):\n",
    "    return DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = load_dataset(\"training_data.txt\", tokenizer)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator(tokenizer),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "### Lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f3f35f7ac847fab68f6125bff410c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119ef8620fef4fa4a0b0663decc82fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4637, 'grad_norm': 44.26801300048828, 'learning_rate': 4.925747720455019e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\utils\\operations.py:156\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: BatchEncoding.to() got an unexpected keyword argument 'non_blocking'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 41\u001b[0m\n\u001b[0;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     35\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     36\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     37\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Fine-tune model\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2427\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2425\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2426\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2427\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2429\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:5045\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5045\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5046\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5047\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\data_loader.py:561\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 561\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m    563\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\utils\\operations.py:158\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# This call is inside the try-block since is_npu_available is not supported by torch.compile.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:818\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor)}\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    820\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:818\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor)}\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    820\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Prepare dataset from list\n",
    "def create_dataset_from_list(texts):\n",
    "    data = {\"text\": texts}\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset.map(\n",
    "        lambda examples: tokenizer(\n",
    "            examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "        ),\n",
    "        batched=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = create_dataset_from_list(sample)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Po prelekcji przewidziana\\nPrzedstawienie przygotowane przez,Pacjent powinien: posiadać pełną zdolność do\\n',\n",
       " 'Po prelekcji przewidziana\\nZachowanie przepisów prawa,wspomniany przepis przejściowy powoduje:',\n",
       " 'Po prelekcji przewidziana\\nDołączył do drużyny, którąPoczątkowo przedsiębiorstwo prowadziło początkowo pod nazwą Pa',\n",
       " 'Po prelekcji przewidziana\\nPan poseł Piechowiak,Prokurator GeneralnyPragnąłbym przedstawić',\n",
       " 'Po prelekcji przewidziana\\nPrzedłożony projekt, popartaPracąPaństwaPrawnicząProponuję']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(prefix, top_k, top_p, temp=0.8, max_length=20):\n",
    "    inputs_ids = tokenizer.encode(prefix, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        inputs_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        temperature=temp,\n",
    "        num_return_sequences=5,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    return [tokenizer.decode(output[i], skip_special_tokens=True) for i in range(5)]\n",
    "\n",
    "\n",
    "def filter_text(generated_texts):\n",
    "\n",
    "    filtered_texts = [\n",
    "        text\n",
    "        for text in generated_texts\n",
    "        if all(word.startswith(generated_texts[0]) for word in text.split())\n",
    "    ]\n",
    "\n",
    "    if not filtered_texts:\n",
    "        return \"No valid text generated.\"\n",
    "\n",
    "    best_text = max(filtered_texts, key=lambda text: len(set(text.split())))\n",
    "    return best_text\n",
    "\n",
    "\n",
    "generate_text(r_prefix, 20, 80)\n",
    "# filtered_text = filter_text(texts)\n",
    "# generate_text\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid text generated.\n"
     ]
    }
   ],
   "source": [
    "def generate_diverse_text(\n",
    "    prefix, model, tokenizer, top_k=50, top_p=0.9, max_length=100, num_variants=5\n",
    "):\n",
    "    input_ids = tokenizer.encode(prefix, return_tensors=\"pt\").to(device)\n",
    "    generated_texts = []\n",
    "\n",
    "    for _ in range(num_variants):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "\n",
    "    # Post-process to ensure all words start with the same letter and proper formatting\n",
    "    filtered_texts = [\n",
    "        text\n",
    "        for text in generated_texts\n",
    "        if all(word.startswith(prefix[0]) for word in text.split())\n",
    "    ]\n",
    "\n",
    "    if not filtered_texts:\n",
    "        return \"No valid text generated.\"\n",
    "\n",
    "    best_text = max(\n",
    "        filtered_texts, key=lambda text: len(set(text.split()))\n",
    "    )  # Example criterion: maximize unique words\n",
    "\n",
    "    return best_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prefix = \"Prawdziwy piekarz przyprawia pieczywo pieprzem\"\n",
    "generated_text = generate_diverse_text(prefix, model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "## Giga skibidi plan działania\n",
    "\n",
    "1. Rodzielam dane na train i validation\n",
    "2. Robie fine tuning\n",
    "3. Dla wylosowanego prefiksu sprawdzam czy nie ma go w train\n",
    "4. Wypluwam rozwiązanie\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
